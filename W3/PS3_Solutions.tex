\documentclass[12pt,letter]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{hyperref}
\onehalfspacing
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\title{Pol Sci 630: Problem Set 3 - Comparisons and Inference - Solutions}

\author{Prepared by: Jan Vogler (\href{mailto:jan.vogler@duke.edu}{jan.vogler@duke.edu})}

\date{Grading Due Date: Friday, September 18th, 12.15 PM (Beginning of Lab)}
 
\maketitle



\section*{R Programming}

\subsection*{Problem 1}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{### Problem 1:}

\hlcom{### a}

\hlstd{x} \hlkwb{=} \hlkwd{seq}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{1000}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{y} \hlkwb{=} \hlnum{2} \hlopt{*} \hlstd{x} \hlopt{-} \hlnum{5}

\hlkwd{cov}\hlstd{(x, y)}
\end{alltt}
\begin{verbatim}
## [1] 166833.3
\end{verbatim}
\end{kframe}
\end{knitrout}

Interpretation: the covariance indicates that there might be a positive linear relationship of x and y. However, because the covariance is not to-scale, the number itself is not very meaningful.

Note: if someone created a different kind of linear function, the result might be a covariance that indicates a negative linear relationship.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{cor}\hlstd{(x, y)}
\end{alltt}
\begin{verbatim}
## [1] 1
\end{verbatim}
\end{kframe}
\end{knitrout}

Interpretation: the correlation is bound between -1 and 1. The correlation value of 1 here means that x and y have a perfect positive linear relationship. As x goes above its mean, y goes above its mean. This is not surprising as we created y through a linear function of x.

Note: if someone created a different kind of linear function, the result might be a correlation value of -1, indicating a perfect negative linear relationship. As x goes above its mean, y goes below its mean.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{noise} \hlkwb{=} \hlkwd{rnorm}\hlstd{(}\hlnum{1000}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{10}\hlstd{)}
\hlstd{y2} \hlkwb{=} \hlstd{y} \hlopt{+} \hlstd{noise}

\hlkwd{cov}\hlstd{(x, y2)}
\end{alltt}
\begin{verbatim}
## [1] 166782.3
\end{verbatim}
\end{kframe}
\end{knitrout}

Interpretation: same as above - the covariance indicates that there might be a linear positive relationship of x and y. However, because the covariance is not to-scale, the number itself is not very meaningful.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{cor}\hlstd{(x, y2)}
\end{alltt}
\begin{verbatim}
## [1] 0.9998485
\end{verbatim}
\end{kframe}
\end{knitrout}

The correlation is bound between -1 and 1. The correlation value here should be very close to 1 or -1 but not be exactly that value due to the random error. As long as some random error has been introduced to a formerly perfect linear relationship, even if that relationship is still generally linear, there will be a reduction in the absolute value of the correlation. Accordingly, the result you can expect to get here is an absolute value of approximately 0.99. The exact value, however, depends on the size of the random error that you introduced. If you introduce a random error that has a greater variance, then the value of your correlation will go down further.

Interpretation: a correlation close to 1 or -1 indicates a nearly perfect linear relationship of two variables. If the relationship is positive, the following is true: as x goes above its mean, y goes above its mean. If the relationship is negative, the following is true: as x goes above its mean, y goes below its mean. The randomly distributed error will in most cases not change the generally strong linear relationship between the two variables.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{### b}

\hlstd{correlation} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{v1}\hlstd{,} \hlkwc{v2}\hlstd{) \{}
    \hlstd{numerator} \hlkwb{=} \hlkwd{sum}\hlstd{((v1} \hlopt{-} \hlkwd{mean}\hlstd{(v1))} \hlopt{*} \hlstd{(v2} \hlopt{-} \hlkwd{mean}\hlstd{(v2)))}\hlopt{/}\hlstd{(}\hlkwd{length}\hlstd{(a)} \hlopt{-} \hlnum{1}\hlstd{)}
    \hlstd{denominator} \hlkwb{=} \hlkwd{sd}\hlstd{(v1)} \hlopt{*} \hlkwd{sd}\hlstd{(v2)}
    \hlkwd{print}\hlstd{(numerator}\hlopt{/}\hlstd{denominator)}
\hlstd{\}}

\hlcom{### Let's try this function.}

\hlstd{a} \hlkwb{=} \hlkwd{seq}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{10}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{noise2} \hlkwb{=} \hlkwd{rnorm}\hlstd{(}\hlnum{10}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{b} \hlkwb{=} \hlstd{a} \hlopt{+} \hlstd{noise2}

\hlkwd{cor}\hlstd{(a, b)}
\end{alltt}
\begin{verbatim}
## [1] 0.967708
\end{verbatim}
\begin{alltt}
\hlkwd{correlation}\hlstd{(a, b)}
\end{alltt}
\begin{verbatim}
## [1] 0.967708
\end{verbatim}
\begin{alltt}
\hlcom{# These two return the same result, meaning that we did it correctly.}

\hlcom{### c}

\hlstd{correlation2} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{v1}\hlstd{,} \hlkwc{v2}\hlstd{) \{}
    \hlkwa{if} \hlstd{(}\hlkwd{length}\hlstd{(v1)} \hlopt{==} \hlkwd{length}\hlstd{(v2)) \{}
        \hlkwa{if} \hlstd{(}\hlkwd{is.numeric}\hlstd{(v1)} \hlopt{&} \hlkwd{is.numeric}\hlstd{(v2)) \{}
            \hlstd{numerator} \hlkwb{=} \hlkwd{sum}\hlstd{((v1} \hlopt{-} \hlkwd{mean}\hlstd{(v1))} \hlopt{*} \hlstd{(v2} \hlopt{-} \hlkwd{mean}\hlstd{(v2)))}\hlopt{/}\hlstd{(}\hlkwd{length}\hlstd{(a)} \hlopt{-}
                \hlnum{1}\hlstd{)}
            \hlstd{denominator} \hlkwb{=} \hlkwd{sd}\hlstd{(v1)} \hlopt{*} \hlkwd{sd}\hlstd{(v2)}
            \hlkwd{print}\hlstd{(numerator}\hlopt{/}\hlstd{denominator)}
        \hlstd{\}} \hlkwa{else} \hlstd{\{}
            \hlkwd{print}\hlstd{(}\hlstr{"The two vectors need to be numeric."}\hlstd{)}
        \hlstd{\}}
    \hlstd{\}} \hlkwa{else} \hlstd{\{}
        \hlkwd{print}\hlstd{(}\hlstr{"The two vectors need to be of the same length."}\hlstd{)}
    \hlstd{\}}
\hlstd{\}}

\hlcom{### Let's plug in vectors that do not work.}

\hlstd{c} \hlkwb{=} \hlkwd{seq}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{11}\hlstd{)}
\hlkwd{length}\hlstd{(c)}
\end{alltt}
\begin{verbatim}
## [1] 11
\end{verbatim}
\begin{alltt}
\hlkwd{correlation2}\hlstd{(a, c)}
\end{alltt}
\begin{verbatim}
## [1] "The two vectors need to be of the same length."
\end{verbatim}
\begin{alltt}
\hlcom{# Returns the correct error message.}

\hlstd{d} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlstr{"a"}\hlstd{,} \hlstr{"b"}\hlstd{,} \hlstr{"c"}\hlstd{,} \hlstr{"d"}\hlstd{,} \hlstr{"e"}\hlstd{,} \hlstr{"f"}\hlstd{,} \hlstr{"g"}\hlstd{,} \hlstr{"h"}\hlstd{,} \hlstr{"i"}\hlstd{,} \hlstr{"j"}\hlstd{)}
\hlkwd{is.numeric}\hlstd{(d)}
\end{alltt}
\begin{verbatim}
## [1] FALSE
\end{verbatim}
\begin{alltt}
\hlkwd{correlation2}\hlstd{(a, d)}
\end{alltt}
\begin{verbatim}
## [1] "The two vectors need to be numeric."
\end{verbatim}
\begin{alltt}
\hlcom{# Returns the correct error message.}
\end{alltt}
\end{kframe}
\end{knitrout}



\subsection*{Problem 2}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{### a}

\hlstd{vec1} \hlkwb{=} \hlkwd{rpois}\hlstd{(}\hlnum{50}\hlstd{,} \hlkwc{lambda} \hlstd{=} \hlnum{10}\hlstd{)}
\hlstd{vec2} \hlkwb{=} \hlkwd{rpois}\hlstd{(}\hlnum{50}\hlstd{,} \hlkwc{lambda} \hlstd{=} \hlnum{12}\hlstd{)}

\hlkwd{t.test}\hlstd{(vec1, vec2)}
\end{alltt}
\begin{verbatim}
## 
## 	Welch Two Sample t-test
## 
## data:  vec1 and vec2
## t = -2.7704, df = 94.302, p-value = 0.006743
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.0899845 -0.5100155
## sample estimates:
## mean of x mean of y 
##     10.36     12.16
\end{verbatim}
\begin{alltt}
\hlcom{### b}

\hlstd{tTestFunction} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{a}\hlstd{,} \hlkwc{b}\hlstd{) \{}
    \hlstd{numerator} \hlkwb{=} \hlkwd{mean}\hlstd{(a)} \hlopt{-} \hlkwd{mean}\hlstd{(b)}
    \hlstd{denominator} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{var}\hlstd{(a)}\hlopt{/}\hlkwd{length}\hlstd{(a)} \hlopt{+} \hlkwd{var}\hlstd{(b)}\hlopt{/}\hlkwd{length}\hlstd{(b))}
    \hlkwd{print}\hlstd{(numerator}\hlopt{/}\hlstd{denominator)}
\hlstd{\}}

\hlkwd{tTestFunction}\hlstd{(vec1, vec2)}
\end{alltt}
\begin{verbatim}
## [1] -2.770415
\end{verbatim}
\begin{alltt}
\hlcom{### c}

\hlstd{tTestFunction2} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{a}\hlstd{,} \hlkwc{b}\hlstd{) \{}
    \hlkwa{if} \hlstd{(}\hlkwd{is.numeric}\hlstd{(a)} \hlopt{&} \hlkwd{is.numeric}\hlstd{(b)) \{}
        \hlstd{numerator} \hlkwb{=} \hlkwd{mean}\hlstd{(a)} \hlopt{-} \hlkwd{mean}\hlstd{(b)}
        \hlstd{denominator} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{var}\hlstd{(a)}\hlopt{/}\hlkwd{length}\hlstd{(a)} \hlopt{+} \hlkwd{var}\hlstd{(b)}\hlopt{/}\hlkwd{length}\hlstd{(b))}
        \hlkwd{print}\hlstd{(numerator}\hlopt{/}\hlstd{denominator)}
    \hlstd{\}} \hlkwa{else} \hlstd{\{}
        \hlkwd{print}\hlstd{(}\hlstr{"Both vectors must be numeric."}\hlstd{)}
    \hlstd{\}}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}



\subsection*{Problem 3}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{### a}

\hlkwd{data}\hlstd{(swiss)}
\hlkwd{summary}\hlstd{(swiss)}
\end{alltt}
\begin{verbatim}
##    Fertility      Agriculture     Examination      Education    
##  Min.   :35.00   Min.   : 1.20   Min.   : 3.00   Min.   : 1.00  
##  1st Qu.:64.70   1st Qu.:35.90   1st Qu.:12.00   1st Qu.: 6.00  
##  Median :70.40   Median :54.10   Median :16.00   Median : 8.00  
##  Mean   :70.14   Mean   :50.66   Mean   :16.49   Mean   :10.98  
##  3rd Qu.:78.45   3rd Qu.:67.65   3rd Qu.:22.00   3rd Qu.:12.00  
##  Max.   :92.50   Max.   :89.70   Max.   :37.00   Max.   :53.00  
##     Catholic       Infant.Mortality
##  Min.   :  2.150   Min.   :10.80   
##  1st Qu.:  5.195   1st Qu.:18.15   
##  Median : 15.140   Median :20.00   
##  Mean   : 41.144   Mean   :19.94   
##  3rd Qu.: 93.125   3rd Qu.:21.70   
##  Max.   :100.000   Max.   :26.60
\end{verbatim}
\begin{alltt}
\hlcom{### b}

\hlstd{lm1} \hlkwb{=} \hlkwd{lm}\hlstd{(Education} \hlopt{~} \hlstd{Fertility} \hlopt{+} \hlstd{Agriculture} \hlopt{+} \hlstd{Examination} \hlopt{+} \hlstd{Catholic} \hlopt{+} \hlstd{Infant.Mortality,}
    \hlkwc{data} \hlstd{= swiss)}

\hlkwd{summary}\hlstd{(lm1)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = Education ~ Fertility + Agriculture + Examination + 
##     Catholic + Infant.Mortality, data = swiss)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.3949  -2.3716  -0.2856   2.8108  11.2985 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(>|t|)    
## (Intercept)      32.74414    8.87888   3.688 0.000657 ***
## Fertility        -0.40851    0.08585  -4.758 2.43e-05 ***
## Agriculture      -0.16242    0.04488  -3.619 0.000804 ***
## Examination       0.41980    0.16339   2.569 0.013922 *  
## Catholic          0.10023    0.02150   4.663 3.29e-05 ***
## Infant.Mortality  0.20408    0.28390   0.719 0.476305    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.907 on 41 degrees of freedom
## Multiple R-squared:  0.7678,	Adjusted R-squared:  0.7395 
## F-statistic: 27.12 on 5 and 41 DF,  p-value: 5.223e-12
\end{verbatim}
\end{kframe}
\end{knitrout}



\subparagraph{c)} In order to get full points on this problem, you need an interpretation for each of the 5 variables.

The interpretation would look like this for Fertility:

There is a negative linear relationship between Fertility and Education. For a 1-point increase in Fertility, we expect a 0.41-point decrease in Education, holding all other variables constant. The t-value is -4.758. This t-value implies a p-value of $2.43*10^{-5}$. This $p < 0.001$ corresponds to a type-1 error rate of $\alpha < 0.001$, meaning that the statistical relationship is significant at all common levels of statistical significance.

The other variables are interpreted accordingly. Agriculture and Catholic are significant at all common levels of statistical significance as well. Please note that Examination is significant at a level of $p < 0.05$, $\alpha < 0.05$, and Infant.Mortality is not significant at common levels of statistical significance. The levels of significance can be found in the tutorial notes.

Answer to the bonus question: What can we say about causality? Nothing really. There are two primary reasons for this:

First and foremost, linear regression does not per se tell us anything about causality - it primarily measures correlation between variables.

Second, we do not have any theory regarding the relationship of Education on the other covariates and so we cannot make any causal claims that are grounded in theory. In particular, there might be a mutual influence between Education and the other variables that we regress it on. This phenomenon is called "endogeneity" and there are various ways to deal with it that you will learn about in the class.

In short, we can't say anything about causality here.



\section*{Probability Theory: Covariance and Correlation}

\subsection*{Problem 4}

\subparagraph{a)} Both X and Y have the uniform distribution over all four points, so each outcome is equally likely.

$\mathbb{E}(X) = \dfrac{1}{4}*(-1) + \dfrac{1}{4}*(0) + \dfrac{1}{4}*(0) + \dfrac{1}{4}*(1) = 0$

\bigskip

$\mathbb{E}(Y) = \dfrac{1}{4}*(0) + \dfrac{1}{4}*(1) + \dfrac{1}{4}*(-1) + \dfrac{1}{4}*(0) = 0$

\bigskip

$\mathbb{E}(X*Y) = \dfrac{1}{4}*(0) + \dfrac{1}{4}*(0) + \dfrac{1}{4}*(0) + \dfrac{1}{4}*(0) = 0$

\bigskip

$Cov(X, Y) = \mathbb{E}(X*Y)-\mathbb{E}(X)*\mathbb{E}(Y) = 0 - 0*0 = 0$

\bigskip

Proof by contradiction. In order to prove that X and Y are not independent, it is sufficient to show that they violate the necessary conditions for independence in one case.

For independence, the following must be true:

$Pr (X=x \cap Y=y) = Pr(X=x)*Pr(Y=y)$

Given this definition, it is sufficient to show that this is not true for some values of X. For example:

$Pr (X=-1 \cap Y=1) = 0$ because this event never occurs.

$Pr (X=-1) = \dfrac{1}{4}$ and $Pr (Y=1) = \dfrac{1}{4}$, meaning that $Pr (X=-1)*Pr(Y=1) = \dfrac{1}{16}$

Accordingly, $Pr (X=-1)*Pr(Y=1) = \dfrac{1}{16} \neq 0 = Pr (X=-1 \cap Y=1)$

\subparagraph{b)} $\mathbb{E}(X) = \dfrac{1}{3}*(-1) + \dfrac{1}{3}*(0) + \dfrac{1}{3}*(1) = 0$

$\mathbb{E}(Y) = \dfrac{1}{3}*(1) + \dfrac{1}{3}*(0) + \dfrac{1}{3}*(1) = \dfrac{2}{3}$

$\mathbb{E}(X*Y) = \dfrac{1}{3}*(-1) + \dfrac{1}{3}*(0) + \dfrac{1}{3}*(1) = 0$

$Cov (X, Y) = \mathbb{E}(X*Y) - \mathbb{E}(X)*\mathbb{E}(Y) = 0 - \dfrac{2}{3} * 0 = 0$

Are X and Y independent? No. We don't need to prove this because we know that $Y=X^2$, so $Y$ was defined to be a function of X. The reason why we don't capture their dependence is that they are not \textit{linearly dependent}. Instead, they are dependent through a quadratic function.

\subparagraph{c)} In order to solve this problem, as in the above problems, we need to calculate the following:

$Cov(X, Y) = \mathbb{E}(X*Y)-\mathbb{E}(X)*\mathbb{E}(Y)$

In order to do this, R is extremely helpful.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{### Problem 3}

\hlcom{### c}

\hlcom{### In order to calculate E(XY), use the following:}

\hlstd{sum} \hlkwb{=} \hlnum{0}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlnum{6}\hlstd{) \{}
    \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlnum{6}\hlstd{) \{}
        \hlstd{sum} \hlkwb{=} \hlstd{(i} \hlopt{+} \hlstd{j)} \hlopt{*} \hlstd{(i} \hlopt{-} \hlstd{j)} \hlopt{+} \hlstd{sum}
    \hlstd{\}}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

This calculation returns the sum of all 36 outcomes, for all possible combinations of the first and the second dice. Each of the above outcomes is equally likely with probability 1/36. We can either multiply every single value by 1/36 or we can, alternatively, simply divide the sum by 36 to get of $E(X*Y)$. The same applies to $E(X)$ and $E(Y)$ below.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{exy} \hlkwb{=} \hlstd{sum}\hlopt{/}\hlnum{36}  \hlcom{# = 0 }
\hlstd{exy}
\end{alltt}
\begin{verbatim}
## [1] 0
\end{verbatim}
\begin{alltt}
\hlcom{### In order to calculate E(X), use the following:}

\hlstd{sum2} \hlkwb{=} \hlnum{0}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlnum{6}\hlstd{) \{}
    \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlnum{6}\hlstd{) \{}
        \hlstd{sum2} \hlkwb{=} \hlstd{(i} \hlopt{+} \hlstd{j)} \hlopt{+} \hlstd{sum2}
    \hlstd{\}}
\hlstd{\}}

\hlstd{ex} \hlkwb{=} \hlstd{sum2}\hlopt{/}\hlnum{36}
\hlstd{ex}
\end{alltt}
\begin{verbatim}
## [1] 7
\end{verbatim}
\begin{alltt}
\hlcom{### In order to calculate E(Y), use the following:}

\hlstd{sum3} \hlkwb{=} \hlnum{0}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlnum{6}\hlstd{) \{}
    \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlnum{6}\hlstd{) \{}
        \hlstd{sum3} \hlkwb{=} \hlstd{(i} \hlopt{-} \hlstd{j)} \hlopt{+} \hlstd{sum3}
    \hlstd{\}}
\hlstd{\}}

\hlstd{ey} \hlkwb{=} \hlstd{sum3}\hlopt{/}\hlnum{36}
\hlstd{ey}
\end{alltt}
\begin{verbatim}
## [1] 0
\end{verbatim}
\begin{alltt}
\hlcom{### The covariance is given by the following formula:}

\hlstd{exy} \hlopt{-} \hlstd{ex} \hlopt{*} \hlstd{ey}  \hlcom{# Returns 0.}
\end{alltt}
\begin{verbatim}
## [1] 0
\end{verbatim}
\begin{alltt}
\hlcom{### Note: We can alternatively use vectorized operations to calculate this}

\hlcom{### For E(X)}

\hlstd{is} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{6}\hlstd{,} \hlnum{6}\hlstd{)}
\hlstd{js} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{6}\hlstd{,} \hlkwc{each} \hlstd{=} \hlnum{6}\hlstd{)}

\hlstd{ex2} \hlkwb{=} \hlkwd{sum}\hlstd{(is} \hlopt{+} \hlstd{js)}\hlopt{/}\hlnum{36}

\hlcom{### For E(Y)}

\hlstd{is} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{6}\hlstd{,} \hlnum{6}\hlstd{)}
\hlstd{js} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{6}\hlstd{,} \hlkwc{each} \hlstd{=} \hlnum{6}\hlstd{)}

\hlstd{ey2} \hlkwb{=} \hlkwd{sum}\hlstd{(is} \hlopt{-} \hlstd{js)}\hlopt{/}\hlnum{36}

\hlcom{### For E(XY)}

\hlstd{exy2} \hlkwb{=} \hlkwd{sum}\hlstd{((is} \hlopt{+} \hlstd{js)} \hlopt{*} \hlstd{(is} \hlopt{-} \hlstd{js))}\hlopt{/}\hlnum{36}

\hlcom{### To get the result, we calculate}

\hlstd{exy2} \hlopt{-} \hlstd{ex2} \hlopt{*} \hlstd{ey2}  \hlcom{# Also returns zero}
\end{alltt}
\begin{verbatim}
## [1] 0
\end{verbatim}
\end{kframe}
\end{knitrout}

Why aren't X and Y independent? Let's try a similar proof by contradiction like above.

$Pr (X=12 \cap Y=1) = 0$ because this event never occurs.

$Pr (X=12) = \dfrac{1}{36}$ and $Pr (Y=1) = \dfrac{4}{36} = \dfrac{1}{9}$, meaning that $Pr (X=12)*Pr(Y=1) = \dfrac{1}{324}$

Accordingly, $Pr (X=12)*Pr(Y=1) = \dfrac{1}{324} \neq 0 = Pr (X=12 \cap Y=1)$



\end{document}
