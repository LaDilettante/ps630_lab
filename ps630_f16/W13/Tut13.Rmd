---
title: "Tutorial 13: Autocorrelation, Error Correction, and Model Simulations"
author: "Jan Vogler (jan.vogler@duke.edu)"
date: "December 2, 2016"
output: pdf_document
---

# Today's Agenda

1. Autocorrelation
2. Panel-corrected standard errors
3. Clustered standard errors
4. Model simulations
5. Using model simulations to estimate substantive effects
6. Learning R - what to do next?



# 1. Autocorrelation

Let us first load the LDC dataset.

```{r,tidy=TRUE}
setwd('C:/Users/Jan/OneDrive/Documents/GitHub/ps630_lab/ps630_f16')
library(foreign)
LDC=read.dta("LDC_IO_replication.dta")
```

We use a regression that we have already seen in the past to illustrate the phenomenon of autocorrelation. This is our standard regression in which we analyze the impact of democratization on tariff levels.

```{r,tidy=TRUE}
lm_basic=lm(newtar ~ l1polity + l1gdp_pc + l1lnpop + l1ecris2 + l1bpc1 + l1avnewtar + factor(ctylabel)-1, data = LDC)
# summary(lm_main)
```

There might be autocorrelation in our model because our units at time t and our units at time t-1 are likely to have similar values for the dependent and independent variables. Let us regress the residuals of our models on the residuals at time t-1:

```{r,tidy=TRUE}
res_t0=lm_basic$resid
res_t1=c(lm_basic$resid[2:length(lm_basic$resid)],NA)
res_data=as.data.frame(cbind(res_t1,res_t0))
head(res_data)
lm_res=lm(res_t1 ~ res_t0, data=res_data)
summary(lm_res)
```

What we can see from this simple regression is that there is a high level of autocorrelation in our errors. Note that this code can easily be extended to include multiple lags of our residuals, which you might want to do to check for autocorrelation for several observations.

To address this problem, if we have enough datapoints available, we can create lags of our dependent variable (and other variables) and then include them in a new regression. The *DataCombine* package is designed for data management, especially time series and panel data. It allows us to easily generate lags of our dependent variable.

install.packages("DataCombine")

```{r,tidy=TRUE}
library(DataCombine)

# summary(LDC)
```

We first define which variables we want to lag and we create a vector that includes prefixes for our lagged variables so that we can easily distinguish them from the non-lagged variables.

```{r,tidy=TRUE}
# Choose the variables you want to lag
toLag = c("newtar","polityiv_update2")

# Define vector with the lag numbers
numberLag = c("lag1_","lag2_","lag3_")

# For loop to lag the data
for (i in 1:3){
  for (lagVar in toLag){
    LDC=slide(LDC, Var=lagVar, # Specify variable to lag
                 TimeVar="date", # Specificy time variable
                 GroupVar="ctylabel", # Unit variable
                 NewVar=paste0(numberLag[i],lagVar), # Name of new variable
                 slideBy = -i, # Lag by how many units, minus -> past
                 keepInvalid = FALSE, # Keep observations for which no lag can be created
                 reminder = TRUE) # Remind you to order the data by group variable
  }
}
```

Let us now create a new model that contains three lags of the dependent variable.

```{r,tidy=TRUE}
lm_lag_dv=lm(newtar ~ lag1_newtar + lag2_newtar + lag3_newtar + l1polity + l1gdp_pc + l1lnpop + l1ecris2 + l1bpc1 + l1avnewtar + factor(ctylabel)-1, data = LDC)

# summary(lm_lag_dv)
```

How can we interpret the results of this model?

Also, there might be a time trend. Tariff levels might be moving up or down over time. If we can capture such a time trend with our model, it might reduce the autocorrelation in our errors.

```{r,tidy=TRUE}
lmTime=lm(newtar ~ lag1_newtar + lag2_newtar + lag3_newtar + l1polity + l1gdp_pc + l1lnpop + l1ecris2 + l1bpc1 + l1avnewtar + date + factor(ctylabel)-1, data = LDC)

# summary(lmTime)
```

How would we interpret the results of our regression?



# 2. Panel-corrected standard errors

When we deal with variables that are correlated over time, i.e. variables that are "sticky" and exhibit little change from year to year, we have to account for this corrrelation somehow. In panel data (cross-sectional, time-series), a standard option is to use panel-corrected standard errors (PCSE).

Let us use a new dataset to illustrate panel-corrected standard errors. This dataset was downloaded from: http://people.stern.nyu.edu/wgreene/Econometrics/PanelDataSets.htm

According to the website it is from the dissertation of Y. Grunfeld (Univ. of Chicago, 1958).

The variables have the following meaning:

Firm = Firm ID, 1,...,10

Year = 1935,...,1954

I = Investment

F = Real Value of the Firm

C = Real Value of the Firm's Capital Stock

```{r,tidy=TRUE}
setwd('C:/Users/Jan/OneDrive/Documents/GitHub/ps630_lab/ps630_f16/w13')
grunfeld=read.csv("grunfeld.csv")
```

Let us first load the "pcse" package: install.packages("pcse")

```{r,tidy=TRUE}
library(pcse)
```

Now let us run a regression that includes panel-corrected standard errors.

We might think that the investment a firm makes is driven by the real value of the firm and the real value of its capital stock. We first have to run the regular regression.

```{r,tidy=TRUE,error=TRUE}
reg1=lm(I ~ F + C, data = grunfeld)
summary(reg1)
```

As we can see, both are positive and highly significant. However, we have not considered the possibility that observations are correlated over time within the same units. That is why we need panel-data error correction.

Let us apply this.

```{r,tidy=TRUE,error=TRUE}
reg1pcse=pcse(reg1, groupN = grunfeld$FIRM, groupT = grunfeld$YEAR, pairwise=T)
```

Now let us obtain a new summary.

```{r,tidy=TRUE}
summary(reg1pcse)
```

Does this differ from the regular regression? What are your conclusions?

By the way, here is how you obtain the corrected variance-covariance matrix. This might be useful for some applications.

```{r,tidy=TRUE}
vcov_pcse=vcovPC(reg1, groupN=grunfeld$FIRM, groupT=grunfeld$YEAR, pairwise=TRUE)
```

Compare this to the regular one.

```{r,tidy=TRUE}
vcov_reg=vcov(reg1)

vcov_pcse
vcov_reg
```



# 3. Clustered standard errors

We use clustered standard errors to correct for correlation among all observations of a specific subgroup of the data.

For example, we might want to use clustered standard errors, when we run regressions on the vote1 dataset that we have used in previous problem sets.

```{r,tidy=TRUE}
setwd('C:/Users/Jan/OneDrive/Documents/GitHub/ps630_lab/ps630_f16/w13')

library(foreign)
vote1=read.dta("VOTE1.dta")
summary(vote1)

lm_vote=lm(voteA ~ expendA + expendB + prtystrA, data = vote1)
summary(lm_vote)
```

In order to use clustered standard errors, we first have to install multiple packages.

install.packages("plm")
install.packages("lmtest")
install.packages("multiwayvcov")

```{r,tidy=TRUE}
library(plm)
library(lmtest)
library(multiwayvcov)
```

Now we have to obtain the corrected variance-covariance matrix.

```{r,tidy=TRUE}
lm_vote.vcovCL=cluster.vcov(lm_vote, vote1$state)

lm_vote_CSE=coeftest(lm_vote, lm_vote.vcovCL)

lm_vote_CSE
```

Have our results changed? How would you interpret these new findings?



# 4. Model simulations

**Credit to Professor Chris Johnston. The code for the simulation approach introduced here is from his class.**

We will look at new data that we have not used previously: data on attitudes toward the United States Supreme Court. This is a topic that students of political behavior and identities might be interested in.

```{r,tidy=TRUE}
setwd('C:/Users/Jan/OneDrive/Documents/GitHub/ps630_lab/ps630_f16/w13')
courtdata <- read.table("courtdata.txt", header=TRUE)
# summary(courtdata)
```

What is the meaning of the variables in the data set?



## Independent Variables

**college** = dummy for college education (1 = college educated)

**ideo** = 5-point ideological self-identification, ranging from "very liberal" (1) to "very conservative" (5)

**soph** = 10-item political knowledge scale (0 = low, 10 = high)

**black & hispanic** = dummies for respective categories (1 = black, 1 = hispanic)

**income** = 15-point household income category scale (1 = low, 15 = high)

**ruling** = dummy indicating respondent correctly identified the Affordable Care Act ruling (1)

**male** = dummy indicating male gender (1 = male)



## Dependent Variable

We are interested in attitudes of US citizens toward the Supreme Court. For this purpose we look at the variable **sclaw** (standing for "Supreme Court Law"). The variable was based on the following statement:

*"The Supreme Court should be allowed to throw out any law it deems unconstitutional"*

The variable ranges from "strongly agree" to "strongly disagree".

Let us estimate a linear model.

```{r,tidy=TRUE}
model1 <- lm(sclaw ~ ruling + ideo + soph + age + male + black + hisp + college + income, data=courtdata)
summary(model1)
```

When we estimate a linear model, all our coefficients are assumed to be normally distributed random variables with a mean and a variance that depends on the data. We can make use of this fact by simulating different versions of the model in which the coefficients are normally distributed around the original model.

For the simulations we need the package "arm". Please make sure to install it via the following command:

install.packages("arm")

```{r,tidy=TRUE}
library(arm)
model1.sims <- sim(model1, n.sims=1000)
```

How did we create these simulations? How could that be useful to us?



# 5. Using model simulations to estimate substantive effects

The following plot is generated by our knowledge about the regression. We access the first coefficient (the intercept) and the third coefficient (ideology). We let ideology vary from 1 to 5. If we plug in the right formula, then we will get the predicted values when all other variables are at the value 0. (This is similar to our predictions from previous tutorials, where we held all variables at their mean.)

```{r,tidy=TRUE}
curve(coef(model1)[1] + coef(model1)[3]*x, from=1, to=5, 
	ylim=c(1,5), xlab="Conservatism", ylab="Opposition to Judicial Review", 
	main="Opposition to Judicial Review as a Function of Ideology", lwd=2)
```

Now let's look instead at the lines that are the result of the 1000 simulations that we created above.

```{r,tidy=TRUE}
curve(coef(model1)[1] + coef(model1)[3]*x, from=1, to=5, 
	ylim=c(1,5), xlab="Conservatism", ylab="Opposition to Judicial Review", 
	main="Opposition to Judicial Review as a Function of Ideology", lwd=2)
for (i in 1:1000){
	curve(coef(model1.sims)[i,1] + coef(model1.sims)[i,3]*x, add=TRUE, col="gray80")
}
```

What we can see in this plot is that the simulated coefficients are very similar to the one that we already had. In fact, the simulation predictions are approximately normally distributed around our original regression line.

Let's run the original command one more time, so we get our regression line back on top of everything else.

```{r,tidy=TRUE}
curve(coef(model1)[1] + coef(model1)[3]*x, from=1, to=5, 
	ylim=c(1,5), xlab="Conservatism", ylab="Opposition to Judicial Review", 
	main="Opposition to Judicial Review as a Function of Ideology", lwd=2)
for (i in 1:1000){
	curve(coef(model1.sims)[i,1] + coef(model1.sims)[i,3]*x, add=TRUE, col="gray80")
}

curve(coef(model1)[1] + coef(model1)[3]*x, col="black", lwd=2, add=TRUE)
```

Let us now utilize our model simulations. They can help us to make an average predictive comparison. We use our model simulations in combination with draws from the data to create a so-called "average predictive comparison". These help us to evaluate the substantive impact that a variable has in comparison with other variables. Our results will include confidence intervals.

In order to do this, we first create an array that we fill with data. We have 1000 simulations of the model and X observations or data points. We will apply each of those 1000 simulations to all our data points to estimate the average predicted effect.

```{r,tidy=TRUE}
d.ideo <- array(NA, c(1000,length(courtdata$sclaw)))
m.ideo <- array(NA, 1000)
```

Now we run a for loop. In this for loop we go through each of the 1000 simulations of the model that we have generated based on the variance-covariance matrix. An in each of the 1000 iterations, we also go through the data in its entirety. In each of these iterations, we subtract the effect of holding our ideology at the minimum from holding it at the maximum. This will give us an idea of what the effect is across many possible models and all of the empirical data that is available to us - a so-called "average prediction" of the effect.

**Please note that the following code applies to all kinds of models, including such that have non-linear link functions. For linear functions, such as linear regression, we can reduce and simplify this code. The full code is provided to ensure that you can also use it with respect to other types of models, especially logit and probit models, where variables generally do not have effects that are directly proportional.**

```{r,tidy=TRUE}
# Remember our model was: sclaw ~ ruling + ideo + soph + age + male + black + hisp + college + income

summary(courtdata$ideo)

for (i in 1:1000){
	d.ideo[i, ] <- (coef(model1.sims)[i,1] + coef(model1.sims)[i,2]*courtdata$ruling + coef(model1.sims)[i,3]*5 +
		coef(model1.sims)[i,4]*courtdata$soph + coef(model1.sims)[i,5]*courtdata$age + coef(model1.sims)[i,6]*courtdata$male
		+ coef(model1.sims)[i,7]*courtdata$black + coef(model1.sims)[i,8]*courtdata$hisp +
		  coef(model1.sims)[i,9]*courtdata$college + coef(model1.sims)[i,10]*courtdata$income) -
	    (coef(model1.sims)[i,1] + coef(model1.sims)[i,2]*courtdata$ruling + coef(model1.sims)[i,3]*0 +
		coef(model1.sims)[i,4]*courtdata$soph + coef(model1.sims)[i,5]*courtdata$age + coef(model1.sims)[i,6]*courtdata$male
		+ coef(model1.sims)[i,7]*courtdata$black + coef(model1.sims)[i,8]*courtdata$hisp +
		  coef(model1.sims)[i,9]*courtdata$college + coef(model1.sims)[i,10]*courtdata$income)
	m.ideo[i] <- mean(d.ideo[i, ])
}

mean(m.ideo)
sd(m.ideo)
quantile(m.ideo, probs=c(.025,.16,.84,.975))
```

Let's compare this to the effect that sophistication has. We need to also estimate the average predictive effects for sophistication. Please note that this time we let ideology vary with the data. What we keep constant here is the sophistication variable at its minimum and maximum to assess the average effect that it has on attitudes toward the Supreme Court. 

```{r,tidy=TRUE}
summary(courtdata$soph)

d.soph <- array(NA, c(1000,length(courtdata$sclaw)))
m.soph <- array(NA, 1000)

for (i in 1:1000){
	d.soph[i, ] <- ((coef(model1.sims)[i,1] + coef(model1.sims)[i,2]*courtdata$ruling + coef(model1.sims)[i,3]*courtdata$ideo +
		coef(model1.sims)[i,4]*10 + coef(model1.sims)[i,5]*courtdata$age + coef(model1.sims)[i,6]*courtdata$male
		+ coef(model1.sims)[i,7]*courtdata$black + coef(model1.sims)[i,8]*courtdata$hisp +
		  coef(model1.sims)[i,9]*courtdata$college + coef(model1.sims)[i,10]*courtdata$income) -
	    (coef(model1.sims)[i,1] + coef(model1.sims)[i,2]*courtdata$ruling + coef(model1.sims)[i,3]*courtdata$ideo +
		coef(model1.sims)[i,4]*0 + coef(model1.sims)[i,5]*courtdata$age + coef(model1.sims)[i,6]*courtdata$male
		+ coef(model1.sims)[i,7]*courtdata$black + coef(model1.sims)[i,8]*courtdata$hisp +
		  coef(model1.sims)[i,9]*courtdata$college + coef(model1.sims)[i,10]*courtdata$income))
	m.soph[i] <- mean(d.soph[i, ])
}

mean(m.soph)
sd(m.soph)
quantile(m.soph, probs=c(.025,.16,.84,.975))
```

Let us plot these two in comparison.

```{r,tidy=TRUE}
plot(1:2, c(mean(m.ideo),mean(m.soph)), type="p", ylim=c(-1.5,0.5), xlab="", 
	main="Ideology, Knowledge, and Attitudes Toward the Supreme Court",
	ylab="Average predictive comparison (min. to max.)", asp=1.5, axes=FALSE)
axis(1, at=c(1,2), labels=c("Ideo.","Know."))
axis(2, at=c(-1.5,-1,-.5,0,.5))
abline(h=0, lty=2)
segments(1, quantile(m.ideo, probs=c(.025)), 1, quantile(m.ideo, probs=c(.975)))
segments(2, quantile(m.soph, probs=c(.025)), 2, quantile(m.soph, probs=c(.975)))
```

Which variable has the greater substantive effect? What can we say about the average influence of the knowledge variable?



# 6. Learning R - what to do next?

Here are some pieces of advice on how to proceed from here.

First, when you have some free time during the winter break, use it to review what you have learned in this class. You will need it in the future.

The following classes are most useful in terms of pushing your R skills further:

1. PolSci 733 - *Maximum Likelihood Methods* (2nd semester)
2. Stats 523 - *Statistical Programming* (3rd semester/5th semester)
3. Stats 601 - *Bayesian and Modern Statistics* (4th semester/6th semester)

Note: R is best learned gradually. Don't take everything at once. Give yourself the opportunity to learn more in the future.

Also, Stats 250 has an R lab, too. This R lab focuses on introducing R for mathematical purposes in the context of statistical theory (and it does not assume a background in R).

The following books will help you to master R:

1. Fox & Weisberg (2010) *An R Companion to Applied Regression* (2nd Edition)
2. Gelman & Hill (2006) *Data Analysis Using Regression and Multilevel/Hierarchical Models*
3. Chang (2013) *R Graphics Cookbook*
4. Kabacoff (2011) *R in Action*
5. Maindonald & Braun (2010) *Data analysis and graphics using R*
6. Matloff (2011) *The Art of R Programming - a Tour of Statistical Software Design*

In the future, you will often find yourself in the situation that the tutorial has not covered a problem you run into. Then you need to be able to find a solution yourself. Taking advanced classes in R will also help you to *develop* a solution yourself, i.e. to write your own script in R to deal with the problem.

R is a continuous learning experience.



## Remember

From our first session.

```{r,tidy=TRUE}
fun=c("R","is","fun")
paste(fun, collapse=" ")
```