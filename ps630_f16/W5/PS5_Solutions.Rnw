\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{color}
\onehalfspacing



\begin{document}

\title{Pol Sci 630: Problem Set 5 - Regression Model Interpretation - Solutions}

\author{Prepared by: Jan Vogler (\href{mailto:jan.vogler@duke.edu}{jan.vogler@duke.edu})}

\date{Grading Due Date: Friday, October 7th, 1.40 PM (Beginning of Lab)}
 
\maketitle



\textbf{\color{red} Insert your comments on the assignment that you are grading above the solution in bold and red text. For example write: ``GRADER COMMENT: everything is correct!" Also briefly point out which, if any, problems were not solved correctly and what the mistake was. See below for more examples.}

\bigskip

\textbf{In order to make your text bold and red, you need to insert the following line at the beginning of the document:}

\begin{verbatim} \usepackage{color} \end{verbatim}

\\ \textbf{and the following lines above the solution of the specific task:}

\begin{verbatim} \textbf{\color{red} GRADER COMMENT: everything is correct!} \end{verbatim}



\pagebreak

\section*{R Programming}

\subsection*{Problem 1}

<<results='show',tidy=TRUE>>=
### a

data(swiss)
summary(swiss)

### b

lm1=lm(Education ~ Fertility + Agriculture + Examination + Catholic + Infant.Mortality, data=swiss)

summary(lm1)
@

\subparagraph{c)} \textbf{In order to get full points on this problem, you need an interpretation for each of the 5 variables.}

The interpretation would look like this for \textit{Fertility}:

There is a negative linear relationship between \textit{Fertility} and \textit{Education}. For a 1-point increase in Fertility, we expect a 0.41-point decrease in Education, holding all other variables constant. The t-value is -4.758. This t-value implies a p-value of $2.43*10^{-5}$. This $p < 0.001$ corresponds to a type-1 error rate of $\alpha < 0.001$, meaning that the statistical relationship is significant at all common levels of statistical significance.

The other variables are interpreted accordingly. \textit{Agriculture} and \textit{Catholic} are significant at all common levels of statistical significance as well. Please note that \textit{Examination} is significant at a level of $p < 0.05$ ($\alpha < 0.05$) and \textit{Infant.Mortality} is not significant at common levels of statistical significance.

The $R^2$ statistic shows us that our model explains 76.78 percent (multiple $R^2$) or 73.95 percent (adjusted $R^2$) of the variation in the dependent variabl. The adjusted $R^2$ is smaller because we are penalized for every variable that is introduced into our model.

The F-statistic shows us that the joint statistical significance of the variables in our model when predicting levels of \textit{Education} is high. With a p-value of $p < 0.001$, our model has an overall predictive capability that is significant at all common levels of statistical significance.

\begin{table}[!htbp] \centering 
  \caption{Regression: Education} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & Education \\ 
\hline \\[-1.8ex] 
 Fertility & $-$0.409$^{***}$ \\ 
  & (0.086) \\ 
  Agriculture & $-$0.162$^{***}$ \\ 
  & (0.045) \\ 
  Examination & 0.420$^{**}$ \\ 
  & (0.163) \\ 
  Catholic & 0.100$^{***}$ \\ 
  & (0.021) \\ 
  Infant.Mortality & 0.204 \\ 
  & (0.284) \\ 
  Constant & 32.744$^{***}$ \\ 
  & (8.879) \\ 
 \hline \\[-1.8ex] 
Observations & 47 \\ 
R$^{2}$ & 0.768 \\ 
Adjusted R$^{2}$ & 0.740 \\ 
Residual Std. Error & 4.907 (df = 41) \\ 
F Statistic & 27.121$^{***}$ (df = 5; 41) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}

\subsection*{Problem 2}

\paragraph{a)} In this task you have to formulate a hypothesis regarding the relationship of one variable and the vote share of party A. For example, you could claim that a higher level of expenditures of party A lead to a higher vote share because with more resources at its disposal, party A can air more advertisements, reach more voters, and mobilize more of its supporters.

Hypothesis: For an increase in the expenditures of party A, we expect an increase in the vote share of party A. (Regardless of which variable you choose, your hypothesis should look similar to this one.)

\paragraph{b)} 

<<results='show',tidy=TRUE>>=
### b
library(foreign)
vote1=read.dta("VOTE1.dta")
summary(vote1)

lm_vote=lm(voteA ~ expendA + expendB + prtystrA, data = vote1)
summary(lm_vote)
@

Let us interpret our findings for the hypothesis above:

\textit{expendA}: For a 1-point increase in the expenditures of party A, we expect a 0.0349 (0.03) increase in the the vote share of party of party A, holding all other variables constant. The associated p-value of $p < 0.001$ means that this relationship is statistically significant at all common levels of statistical significance. This means that there is strong support  for the hypothesis that increases in the expenditures of A increase its vote share.

The $R^2$ statistic shows us that our model explains 56.87 percent (multiple $R^2$) or 56.1 percent (adjusted $R^2$) of the variation in the dependent variable. The adjusted $R^2$ is smaller because we are penalized for every variable that is introduced into our model.

The F-statistic shows us that the joint statistical significance of the variables in our model when predicting levels of FDI inflows is high. With a p-value of $p < 0.001$, our model has an overall predictive capability that is significant at all common levels of statistical significance.

\paragraph{c)} 

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & voteA \\ 
\hline \\[-1.8ex] 
 expendA & 0.035$^{***}$ \\ 
  & (0.003) \\ 
  & \\ 
 expendB & $-$0.035$^{***}$ \\ 
  & (0.003) \\ 
  & \\ 
 prtystrA & 0.343$^{***}$ \\ 
  & (0.088) \\ 
  & \\ 
 Constant & 33.267$^{***}$ \\ 
  & (4.417) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & 173 \\ 
R$^{2}$ & 0.569 \\ 
Adjusted R$^{2}$ & 0.561 \\ 
Residual Std. Error & 11.121 (df = 169) \\ 
F Statistic & 74.267$^{***}$ (df = 3; 169) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

\paragraph{d)}

Finally, we show the effect graphically:

<<results='show',tidy=TRUE>>=
### We create a new dataframe with the average values for every variable and vary Polity IV
nd <- data.frame(expendA=seq(min(vote1$expendA),max(vote1$expendA),length.out=11), expendB=rep(mean(vote1$expendB),11), prtystrA=rep(mean(vote1$prtystrA),11))

pred.p1 <- predict(lm_vote, type="response", se.fit=TRUE, newdata=nd)

pred.table <- cbind(pred.p1$fit, pred.p1$se.fit)

fit <- pred.p1$fit
low <- pred.p1$fit - 2*pred.p1$se.fit
high <- pred.p1$fit + 2*pred.p1$se.fit
cis <- cbind(fit, low, high)

cis ### To extract the values

plot(pred.p1$fit, type="l", ylim=c(35,100), main="Incumbent expenditures and vote share", 
     xlab="Incumbent expenditures", ylab="Vote share", axes=FALSE)
axis(1, at=seq(1,11), labels=round(seq(min(vote1$expendA),max(vote1$expendA),length.out=11)))
axis(2, at=seq(35,100,by=5), labels=seq(35,100,by=5))
matlines(cis[,c(2,3)], lty=2, col="black")
@



\section*{Linear Regression Models Interpretation Questions}

\subsection*{Problem 3}

\paragraph*{a)} If we do not include polynomials of higher order, OLS regression can adequately model only \textbf{linear} relationships between one dependent variable (response variable) and one or multiple independent variables (predictor variables).

The reason for why we can only model linear relationships is that our model assumes that for every independent variable there is only a single slope coefficient that is constant for all values of that independent variable. In other words, it assumes direct proportionality. The additional inclusion of polynomials of higher order would allow us to have different slopes at different values of the independent variable.

\paragraph*{b)} OLS regression does not per se tell us anything about causality. OLS regression primarily measures linear relationships between two variables and can give us an answer to the question how to variables are correlated with each other. However, without a strong theory, OLS does not allow us to make statements regarding causality.

There are several reasons for this. First, there could be reverse causality, meaning that the response variable in our model has a causal effect on the predictor variable. Second, there could be endogeneity, meaning that there is mutual causal influence of response and predictor variables. Third, there could be omitted variable bias, meaning that a third variable influences both the predictor and the response variable. Fourth, there could be ``parallel trends", meaning that although two variables have no relationship to each other, they only move simultaneously. For example, two variables could both increase over time for completeley different reasons, potentially giving the wrong impression that they are causally connected. These are the four main reasons why we should not per se view the results of a linear regression as reflecting causality.

\paragraph*{c)} The Residual Sum of Squares (RSS) can be found through the following calculation involving the Root Mean Squared Error (RMSE):

\bigskip

RSS = $(RMSE^2)*(Degrees\ of \ Freedom)$

\bigskip

Furthermore, given that we know $R^2$, once we know the RSS, we can use the fact that $R^2 = 1 - \dfrac{RSS}{TSS}$ to find that:

\bigskip

$\dfrac{1 - R^2}{RSS} = \dfrac{1}{TSS}$ \rightarrow $\dfrac{RSS}{1 - R^2} = TSS$

\bigskip

Once we know both the TSS and the RSS, we can easily calculate the RegSS, since the $RegSS = TSS - RSS$.



\section*{Statistical Theory: Linear Regression Models}

\subsection*{Problem 4}

\subparagraph{a)} The definition of $\beta_1$ is $\dfrac{Cov(X,Y)}{Var(X)}$. The definition of $\beta_1'$ is $\dfrac{Cov(Y,X)}{Var(Y)}.

The formula for variance is $\dfrac{\sum{(x_i - \bar{x})}}{n-1}$. Assuming that the variables X and Y have two or more different values, their variances are always positive. Therefore, the denominator in both equations is positive.

If the variance is always positive, then the sign of the covariance (+, -, or 0) determines the sign of $\beta_1$ and $\beta_1'$. In the following equations, + stands for a positive number, - for a negative number, and 0 for zero.

$\beta_1 = \dfrac{Cov(X,Y)}{+}$ and $\beta_1' = \dfrac{Cov(Y,X)}{+}$

Since $Cov(X,Y) = Cov(Y,X)$, the following is true:

If $Cov(X,Y) > 0$, then $\beta_1 = \dfrac{+}{+}$ and $\beta_1' = \dfrac{+}{+}$, meaning that both are positive.

\bigskip

If $Cov(X,Y) < 0$, then $\beta_1 = \dfrac{-}{+}$ and $\beta_1' = \dfrac{-}{+}$, meaning that both are negative.

\bigskip

If $Cov(X,Y) = 0$, then $\beta_1 = \dfrac{0}{+}$ and $\beta_1' = \dfrac{0}{+}$, meaning that both are zero.

\bigskip

It follows that $\beta_1$ and $\beta_1'$ always have the same sign.

\subparagraph{b)} The intercepts, $\beta_0$ and $\beta_0'$, do not always have the same sign as we can show with a simple counter example (proof by contradiction).

Assume that the variables X and Y have the following values:

\begin{center}
 \begin{tabular}{||c c||} 
 \hline
 X & Y \\ [0.5ex] 
 \hline\hline
 0 & 2 \\ 
 \hline
 1 & 3 \\
 \hline
 2 & 4 \\
 \hline
 3 & 5 \\
  \hline
\end{tabular}
\end{center}

Note that in this case $Y = 2 + X$ and $X = -2 + Y$, so $\beta_0$ = 2 and $\beta_0'$ = -2. In this case $\beta_0$ and $\beta_0'$ have different signs, meaning that the statement is not true.



\end{document}